b'Preprint\nBAXBENCH: CAN LLMS GENERATE\nCORRECT AND SECURE BACKENDS?\nMark Vero1, Niels M\xc3\xbcndler1, Victor Chibotaru2, Veselin Raychev2, Maximilian Baader1,\nNikola Jovanovi\xc2\xb4c1, Jingxuan He3, Martin Vechev1\n1ETH Zurich, 2LogicStar.ai, 3UC Berkeley\n{mark.vero,niels.muendler}@inf.ethz.ch, {chibo,veselin}@logicstar.ai\nABSTRACT\nThe automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits, and\nsolve algorithmic coding tasks. However, to achieve full automation, LLMs should\nbe able to generate production-quality, self-contained application modules. To eval-\nuate the capabilities of LLMs in solving this challenge, we introduce BAXBENCH,\na novel evaluation benchmark consisting of 392 tasks for the generation of backend\napplications. We focus on backends for three critical reasons: (i) they are practically\nrelevant, building the core components of most modern web and cloud software, (ii)\nthey are difficult to get right, requiring multiple functions and files to achieve the\ndesired functionality, and (iii) they are security-critical, as they are exposed to un-\ntrusted third-parties, making secure solutions that prevent deployment-time attacks\nan imperative. BAXBENCH validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by executing\nend-to-end exploits. Our experiments reveal key limitations of current LLMs in\nboth functionality and security: (i) even the best model, OpenAI o1, achieves a\nmere 60% on code correctness; (ii) on average, we could successfully execute\nsecurity exploits on more than half of the correct programs generated by each LLM;\nand (iii) in less popular backend frameworks, models further struggle to generate\ncorrect and secure applications. Progress on BAXBENCH signifies important steps\ntowards autonomous and secure software development with LLMs1.\n1\nINTRODUCTION\nOpenAI\no3-mini\nClaude\n3.5 Sonnet\nDeepSeek\nR1\nOpenAI\no1\n0%\n50%\n100%\n35%\nIncorrect\nCorrect,\nInsecure\nCorrect,\nSecure\nFigure 1: Even flagship models struggle to\ngenerate correct and secure application back-\nends, signifying that LLMs are not yet ready for\ndeployment-ready coding automation.\nAutomating software development is a key aspi-\nrational goal of Large Language Models (LLMs),\npromising to revolutionize the software industry\n(Lyu et al., 2024).\nThey have shown impres-\nsive capabilities in assisting developers by gen-\nerating function-level completions (Chen et al.,\n2021; Austin et al., 2021a), suggesting code patches\n(Jimenez et al., 2024), and solving algorithmic prob-\nlems (Hendrycks et al., 2021). However, it remains\nunclear if LLMs are ready to autonomously gener-\nate larger-scale, deployment-ready code.\nThe Gap in LLM Code Benchmarking\nThis\ngap in understanding LLMs\xe2\x80\x99 capabilities is also\nreflected in the current state of LLM benchmarking.\nNamely, most current coding benchmarks assess LLMs\xe2\x80\x99 capabilities at function-level code writing\nand bug fixing (Chen et al., 2021; Austin et al., 2021a; Muennighoff et al., 2023), or focus on specific\n1Code and dataset are available at: https://baxbench.com/\n1\narXiv:2502.11844v1  [cs.CR]  17 Feb 2025\n'
